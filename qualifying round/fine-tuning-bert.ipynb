{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:41:48.322399Z",
     "start_time": "2024-03-27T10:41:44.224645Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, Dataset\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (4.30.0)\r\n",
      "Requirement already satisfied: datasets in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (2.17.1)\r\n",
      "Collecting evaluate\r\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Collecting accelerate\r\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: filelock in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (3.9.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (0.20.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (1.23.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (4.64.1)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from datasets) (15.0.0)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from datasets) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from datasets) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from datasets) (1.5.3)\r\n",
      "Requirement already satisfied: xxhash in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from datasets) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from datasets) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from datasets) (3.9.3)\r\n",
      "Collecting responses<0.19 (from evaluate)\r\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\r\n",
      "Requirement already satisfied: psutil in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from accelerate) (5.9.8)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from accelerate) (2.2.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.9.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\r\n",
      "Requirement already satisfied: sympy in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m290.1/290.1 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\r\n",
      "Installing collected packages: responses, accelerate, evaluate\r\n",
      "Successfully installed accelerate-0.28.0 evaluate-0.4.1 responses-0.18.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate accelerate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:41:51.381270Z",
     "start_time": "2024-03-27T10:41:48.323314Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:27:51.109918Z",
     "start_time": "2024-03-27T14:27:51.105781Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/first_task/train.csv')\n",
    "test = pd.read_csv('../data/first_task/test.csv')\n",
    "sample = pd.read_csv('../data/first_task/sample_submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:46:01.357511Z",
     "start_time": "2024-03-27T10:45:58.149655Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_text, test_text, train_label, test_label = train_test_split(train['text'], train['sentiment'], test_size=0.2,\n",
    "                                                                  random_state=42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:46:03.290394Z",
     "start_time": "2024-03-27T10:46:03.055961Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9006aa7258b4ae28041650aadb0051a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eae44ee5c4084c5ab6765dcc5af7a64b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "624a013efcf34a41b608f1a0118dc2ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29ad938c21224be2b595d2f2a96ed701"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"papluca/xlm-roberta-base-language-detection\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:47:52.060972Z",
     "start_time": "2024-03-27T10:47:43.594005Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def preprocess_function(text):\n",
    "    return tokenizer(text, truncation=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:55:38.420262Z",
     "start_time": "2024-03-27T10:55:38.413208Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "tokenized_train = train['text'].apply(preprocess_function)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:58:10.735473Z",
     "start_time": "2024-03-27T10:56:38.605203Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T10:58:46.083908Z",
     "start_time": "2024-03-27T10:58:46.082103Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "f1 = evaluate.load(\"f1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:00:15.669898Z",
     "start_time": "2024-03-27T11:00:14.545676Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return f1.compute(predictions=predictions, references=labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:00:22.078430Z",
     "start_time": "2024-03-27T11:00:22.072899Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "id2label = {0: \"NEUTRAL\", 1: \"POSITIVE\", 2: \"NEGATIVE\"}\n",
    "label2id = {\"NEUTRAL\": 0, \"POSITIVE\": 1, \"NEGATIVE\": 2}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:01:42.107421Z",
     "start_time": "2024-03-27T11:01:42.102904Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at papluca/xlm-roberta-base-language-detection and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([20]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([20, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_ckpt, num_labels=3, id2label=id2label, label2id=label2id,ignore_mismatched_sizes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T11:24:21.331478Z",
     "start_time": "2024-03-27T11:24:16.218121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:14:53.564547Z",
     "start_time": "2024-03-27T14:14:53.559642Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(7075) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (0.28.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from accelerate) (1.23.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from accelerate) (23.2)\r\n",
      "Requirement already satisfied: psutil in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from accelerate) (5.9.8)\r\n",
      "Requirement already satisfied: pyyaml in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from accelerate) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from accelerate) (2.2.1)\r\n",
      "Requirement already satisfied: huggingface-hub in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from accelerate) (0.20.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from accelerate) (0.4.2)\r\n",
      "Requirement already satisfied: filelock in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.9.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\r\n",
      "Requirement already satisfied: requests in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.64.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:15:29.155536Z",
     "start_time": "2024-03-27T14:15:26.852766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m training_args \u001B[38;5;241m=\u001B[39m \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../models/bert.pkl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_eval_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevaluation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mepoch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mepoch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mload_best_model_at_end\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m     11\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     14\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     15\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     19\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics,\n\u001B[1;32m     20\u001B[0m )\n",
      "File \u001B[0;32m<string>:111\u001B[0m, in \u001B[0;36m__init__\u001B[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001B[0m\n",
      "File \u001B[0;32m~/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/transformers/training_args.py:1340\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1334\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(version\u001B[38;5;241m.\u001B[39mparse(torch\u001B[38;5;241m.\u001B[39m__version__)\u001B[38;5;241m.\u001B[39mbase_version) \u001B[38;5;241m==\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.0.0\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16:\n\u001B[1;32m   1335\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1337\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1338\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1339\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[0;32m-> 1340\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1341\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (get_xla_device_type(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPU\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1342\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16_full_eval)\n\u001B[1;32m   1343\u001B[0m ):\n\u001B[1;32m   1344\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1345\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1346\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (`--fp16_full_eval`) can only be used on CUDA devices.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1347\u001B[0m     )\n\u001B[1;32m   1349\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1351\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1356\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16_full_eval)\n\u001B[1;32m   1357\u001B[0m ):\n",
      "File \u001B[0;32m~/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/transformers/training_args.py:1764\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1760\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1761\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n\u001B[1;32m   1762\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1763\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m-> 1764\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_devices\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/transformers/utils/generic.py:54\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[0;34m(self, obj, objtype)\u001B[0m\n\u001B[1;32m     52\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 54\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
      "File \u001B[0;32m~/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/transformers/training_args.py:1672\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1670\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[1;32m   1671\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available(min_version\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.20.1\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1672\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[1;32m   1673\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1674\u001B[0m         )\n\u001B[1;32m   1675\u001B[0m     AcceleratorState\u001B[38;5;241m.\u001B[39m_reset_state(reset_partial_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   1676\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistributed_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mImportError\u001B[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/bert.pkl\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:15:30.110131Z",
     "start_time": "2024-03-27T14:15:30.047297Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "82633     Отель расположен очень удачно(на мой взгляд)-в...\n54230     Великолепное обслуживание. Сотрудники ресепшен...\n121119    Пора уже обнародовать скрываемые властями кост...\n760       Коррупция в неврологическом отделении Я просто...\n132950    отличный отель в современном стиле, есть небол...\n                                ...                        \n119879    Отмечали день рождения небольшой компании , вс...\n103694    Номер был с окнами во двор, поэтому было тихо ...\n131932    мы с подругой учимся рядом - потому и забегаем...\n146867    Возможно, у нас бывает, что на внутренний рыно...\n121958    Спасибо Прошли 2курса лечения на дневном стаци...\nName: text, Length: 151912, dtype: object"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T20:42:45.670Z",
     "start_time": "2024-03-24T20:42:45.662610Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-24T20:42:50.917611Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__len__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels)\n\u001B[0;32m---> 13\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m Data(\u001B[43mtokens_train\u001B[49m, train_label)\n\u001B[1;32m     14\u001B[0m test_dataset \u001B[38;5;241m=\u001B[39m Data(tokens_test, test_label)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokens_train' is not defined"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T20:50:28.180896Z",
     "start_time": "2024-03-24T20:50:28.127074Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {'F1': f1}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T20:50:41.054578Z",
     "start_time": "2024-03-24T20:50:41.047890Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  #Выходной каталог\n",
    "    num_train_epochs=3,  #Кол-во эпох для обучения\n",
    "    per_device_train_batch_size=8,  #Размер пакета для каждого устройства во время обучения\n",
    "    per_device_eval_batch_size=8,  #Размер пакета для каждого устройства во время валидации\n",
    "    weight_decay=0.01,  #Понижение весов\n",
    "    logging_dir='./logs',  #Каталог для хранения журналов\n",
    "    load_best_model_at_end=True,  #Загружать ли лучшую модель после обучения\n",
    "    learning_rate=1e-5,  #Скорость обучения\n",
    "    evaluation_strategy='epoch',  #Валидация после каждой эпохи (можно сделать после конкретного кол-ва шагов)\n",
    "    logging_strategy='epoch',  #Логирование после каждой эпохи\n",
    "    save_strategy='epoch',  #Сохранение после каждой эпохи\n",
    "    save_total_limit=1,\n",
    "    seed=21)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  tokenizer=tokenizer,\n",
    "                  args=training_args,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=train_dataset,\n",
    "                  compute_metrics=compute_metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path = \"fine-tune-bert\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_prediction():\n",
    "    test_pred = trainer.predict(test_dataset)\n",
    "    labels = np.argmax(test_pred.predictions, axis=-1)\n",
    "    return labels\n",
    "\n",
    "\n",
    "pred = get_prediction()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
