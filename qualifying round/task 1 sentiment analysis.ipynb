{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from nltk) (1.3.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from nltk) (2023.12.25)\r\n",
      "Requirement already satisfied: tqdm in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from nltk) (4.64.1)\r\n",
      "Requirement already satisfied: transformers in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (4.30.0)\r\n",
      "Requirement already satisfied: filelock in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (3.9.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (0.20.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (1.23.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (23.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (0.13.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from transformers) (4.64.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.10.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.9.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\r\n",
      "Requirement already satisfied: razdel in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (0.5.0)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (1.4.0)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from scikit-learn) (1.23.5)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from scikit-learn) (1.10.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from scikit-learn) (3.4.0)\r\n",
      "Requirement already satisfied: natasha in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (1.6.0)\r\n",
      "Requirement already satisfied: pymorphy2 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from natasha) (0.9.1)\r\n",
      "Requirement already satisfied: razdel>=0.5.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from natasha) (0.5.0)\r\n",
      "Requirement already satisfied: navec>=0.9.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from natasha) (0.10.0)\r\n",
      "Requirement already satisfied: slovnet>=0.6.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from natasha) (0.6.0)\r\n",
      "Requirement already satisfied: yargy>=0.16.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from natasha) (0.16.0)\r\n",
      "Requirement already satisfied: ipymarkup>=0.8.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from natasha) (0.9.0)\r\n",
      "Requirement already satisfied: intervaltree>=3 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\r\n",
      "Requirement already satisfied: numpy in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from navec>=0.9.0->natasha) (1.23.5)\r\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from pymorphy2->natasha) (0.7.2)\r\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from pymorphy2->natasha) (2.4.417127.4579844)\r\n",
      "Requirement already satisfied: docopt>=0.6 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from pymorphy2->natasha) (0.6.2)\r\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install razdel\n",
    "!pip install scikit-learn\n",
    "!pip install natasha"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T19:40:12.134151Z",
     "start_time": "2024-03-24T19:40:05.187396Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:16:53.744461Z",
     "start_time": "2024-03-25T19:16:52.630521Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Посмотрим на предоставленные датасеты"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/first_task/train.csv', encoding='utf-8')\n",
    "test = pd.read_csv('../data/first_task/test.csv')\n",
    "sample = pd.read_csv('../data/first_task/sample_submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:23:19.873187Z",
     "start_time": "2024-03-25T19:23:16.290167Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 189891 entries, 0 to 189890\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   ID         189891 non-null  int64 \n",
      " 1   text       189891 non-null  object\n",
      " 2   sentiment  189891 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 4.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T19:40:14.601593Z",
     "start_time": "2024-03-24T19:40:14.562190Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21098 entries, 0 to 21097\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   ID      21098 non-null  int64 \n",
      " 1   text    21098 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 329.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T19:40:14.611372Z",
     "start_time": "2024-03-24T19:40:14.607251Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "          ID  sentiment\n0          0          0\n1          1          2\n2          2          2\n3          3          2\n4          4          1\n...      ...        ...\n21093  21093          0\n21094  21094          0\n21095  21095          0\n21096  21096          0\n21097  21097          1\n\n[21098 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21093</th>\n      <td>21093</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21094</th>\n      <td>21094</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21095</th>\n      <td>21095</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21096</th>\n      <td>21096</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21097</th>\n      <td>21097</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>21098 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T19:40:14.630420Z",
     "start_time": "2024-03-24T19:40:14.613550Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().any().any()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T19:40:14.852923Z",
     "start_time": "2024-03-24T19:40:14.621039Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: xlabel='sentiment', ylabel='count'>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='sentiment', data=train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T19:40:15.594793Z",
     "start_time": "2024-03-24T19:40:14.706592Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:17:04.682790Z",
     "start_time": "2024-03-25T19:17:04.653141Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Важный этап - предобработка текстовых данных"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/antontagiev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:17:12.497294Z",
     "start_time": "2024-03-25T19:17:11.050591Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Токенизируем текст и удалим знаки препинания и ссылки, чтобы не подавать в модель лишний шум"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import razdel\n",
    "import re\n",
    "\n",
    "\n",
    "# Функция для токенизации текста с использованием библиотеки razdel\n",
    "def tokenize_text(text):\n",
    "    tokens = [token.text for token in razdel.tokenize(text)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "pattern = r'[^\\w\\s]|\\d|http\\S+'\n",
    "\n",
    "# Применяем регулярное выражение ко всему столбцу 'Text'\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(pattern, '', x))\n",
    "train['text'] = train['text'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))\n",
    "# Применяем токенизацию сначала к тексту, а затем к результатам первой токенизации\n",
    "train['refact_text'] = train['text'].apply(lambda tokens: ''.join(tokens))  # объединяем списки токенов обратно в текст\n",
    "train['refact_text'] = train['refact_text'].apply(tokenize_text)  # применяем токенизацию с использованием razdel\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:34:14.698521Z",
     "start_time": "2024-03-25T19:33:30.907685Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "train['refact_text'] = train['refact_text'].apply(lambda x: ' '.join(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:36:32.370575Z",
     "start_time": "2024-03-25T19:36:30.753159Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "0         си спросил его о Посланник АллахаТы порицаешь ...\n1         Роднее всех родных Попала я в ГКБ еще в декабр...\n2         Непорядочное отношение к своим работникам Рабо...\n3         Отсутствуют нормативы Госты и прочее что позво...\n4         У меня машина в руках лет и это первая моя маш...\n                                ...                        \n189886    Мой юбилей я отмечал в ресторане Астория Этот ...\n189887    Отлично встретили разместили в роскошном номер...\n189888    Была в Васаби на ст метро Сенная Во первых рес...\n189889    Ребята не стоит смотреть этот фильм Вы молодые...\n189890    Про махание руками нигде не нашел Почему сегод...\nName: refact_text, Length: 189891, dtype: object"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['refact_text']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:36:43.013973Z",
     "start_time": "2024-03-25T19:36:43.000731Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "{'а',\n 'без',\n 'более',\n 'больше',\n 'будет',\n 'будто',\n 'бы',\n 'был',\n 'была',\n 'были',\n 'было',\n 'быть',\n 'в',\n 'вам',\n 'вас',\n 'вдруг',\n 'ведь',\n 'во',\n 'вот',\n 'впрочем',\n 'все',\n 'всегда',\n 'всего',\n 'всех',\n 'всю',\n 'вы',\n 'где',\n 'да',\n 'даже',\n 'два',\n 'для',\n 'до',\n 'другой',\n 'его',\n 'ее',\n 'ей',\n 'ему',\n 'если',\n 'есть',\n 'еще',\n 'ж',\n 'же',\n 'за',\n 'зачем',\n 'здесь',\n 'и',\n 'из',\n 'или',\n 'им',\n 'иногда',\n 'их',\n 'к',\n 'как',\n 'какая',\n 'какой',\n 'когда',\n 'конечно',\n 'кто',\n 'куда',\n 'ли',\n 'лучше',\n 'между',\n 'меня',\n 'мне',\n 'много',\n 'может',\n 'можно',\n 'мой',\n 'моя',\n 'мы',\n 'на',\n 'над',\n 'надо',\n 'наконец',\n 'нас',\n 'не',\n 'него',\n 'нее',\n 'ней',\n 'нельзя',\n 'нет',\n 'ни',\n 'нибудь',\n 'никогда',\n 'ним',\n 'них',\n 'ничего',\n 'но',\n 'ну',\n 'о',\n 'об',\n 'один',\n 'он',\n 'она',\n 'они',\n 'опять',\n 'от',\n 'перед',\n 'по',\n 'под',\n 'после',\n 'потом',\n 'потому',\n 'почти',\n 'при',\n 'про',\n 'раз',\n 'разве',\n 'с',\n 'сам',\n 'свою',\n 'себе',\n 'себя',\n 'сейчас',\n 'со',\n 'совсем',\n 'так',\n 'такой',\n 'там',\n 'тебя',\n 'тем',\n 'теперь',\n 'то',\n 'тогда',\n 'того',\n 'тоже',\n 'только',\n 'том',\n 'тот',\n 'три',\n 'тут',\n 'ты',\n 'у',\n 'уж',\n 'уже',\n 'хорошо',\n 'хоть',\n 'чего',\n 'чем',\n 'через',\n 'что',\n 'чтоб',\n 'чтобы',\n 'чуть',\n 'эти',\n 'этого',\n 'этой',\n 'этом',\n 'этот',\n 'эту',\n 'я'}"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:36:34.600710Z",
     "start_time": "2024-03-25T19:36:34.591805Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Аналогично удалим стоп-слова и приведем текст к нижнему регистру. Стоп-слова - слова, часто встречающиеся в языке и зачастую не несущие за собой много смысла, то есть зашумляющие модель"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = text.split()  # Разбиваем текст на слова\n",
    "    filtered_words = [word.lower() for word in words if\n",
    "                      word.lower() not in stop_words]  # Фильтруем слова, игнорируя регистр\n",
    "    return ' '.join(filtered_words)  # Собираем текст обратно\n",
    "\n",
    "\n",
    "train['without_stopwords'] = train['refact_text'].apply(remove_stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:41:01.473963Z",
     "start_time": "2024-03-25T19:40:58.243675Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['refact_text'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrefact_text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[1;32m    330\u001B[0m     )\n\u001B[0;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/pandas/core/frame.py:5399\u001B[0m, in \u001B[0;36mDataFrame.drop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   5251\u001B[0m \u001B[38;5;129m@deprecate_nonkeyword_arguments\u001B[39m(version\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, allowed_args\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m   5252\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(  \u001B[38;5;66;03m# type: ignore[override]\u001B[39;00m\n\u001B[1;32m   5253\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5260\u001B[0m     errors: IgnoreRaise \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5261\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5262\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   5263\u001B[0m \u001B[38;5;124;03m    Drop specified labels from rows or columns.\u001B[39;00m\n\u001B[1;32m   5264\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5397\u001B[0m \u001B[38;5;124;03m            weight  1.0     0.8\u001B[39;00m\n\u001B[1;32m   5398\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 5399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5400\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5401\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5402\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5403\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5405\u001B[0m \u001B[43m        \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5406\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5407\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[1;32m    330\u001B[0m     )\n\u001B[0;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/pandas/core/generic.py:4505\u001B[0m, in \u001B[0;36mNDFrame.drop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   4503\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m axis, labels \u001B[38;5;129;01min\u001B[39;00m axes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m   4504\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 4505\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_drop_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4507\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[1;32m   4508\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_inplace(obj)\n",
      "File \u001B[0;32m~/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/pandas/core/generic.py:4546\u001B[0m, in \u001B[0;36mNDFrame._drop_axis\u001B[0;34m(self, labels, axis, level, errors, only_slice)\u001B[0m\n\u001B[1;32m   4544\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mdrop(labels, level\u001B[38;5;241m=\u001B[39mlevel, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[1;32m   4545\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4546\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m \u001B[43maxis\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4547\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mget_indexer(new_axis)\n\u001B[1;32m   4549\u001B[0m \u001B[38;5;66;03m# Case for non-unique axis\u001B[39;00m\n\u001B[1;32m   4550\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6934\u001B[0m, in \u001B[0;36mIndex.drop\u001B[0;34m(self, labels, errors)\u001B[0m\n\u001B[1;32m   6932\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39many():\n\u001B[1;32m   6933\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 6934\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(labels[mask])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in axis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6935\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m indexer[\u001B[38;5;241m~\u001B[39mmask]\n\u001B[1;32m   6936\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelete(indexer)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"['refact_text'] not found in axis\""
     ]
    }
   ],
   "source": [
    "train = train.drop('refact_text', axis=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:41:20.725659Z",
     "start_time": "2024-03-25T19:41:20.665824Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "            ID                                               text  sentiment  \\\n0        21098  си спросил его о Посланник АллахаТы порицаешь ...          1   \n1        21099  Роднее всех родных Попала я в ГКБ еще в декабр...          1   \n2        21100  Непорядочное отношение к своим работникам Рабо...          2   \n3        21101  Отсутствуют нормативы Госты и прочее что позво...          1   \n4        21102  У меня машина в руках лет и это первая моя маш...          1   \n...        ...                                                ...        ...   \n189886  210984  Мой юбилей я отмечал в ресторане Астория Этот ...          2   \n189887  210985  Отлично встретили разместили в роскошном номер...          1   \n189888  210986  Была в Васаби на ст метро Сенная Во первых рес...          0   \n189889  210987  Ребята не стоит смотреть этот фильм Вы молодые...          0   \n189890  210988  Про махание руками нигде не нашел Почему сегод...          1   \n\n                                        without_stopwords  \n0       си спросил посланник аллахаты порицаешь чтото ...  \n1       роднее родных попала гкб декабре г ехать больн...  \n2       непорядочное отношение своим работникам работа...  \n3       отсутствуют нормативы госты прочее позволило о...  \n4       машина руках лет это первая машина могу никак ...  \n...                                                   ...  \n189886  юбилей отмечал ресторане астория ресторан поль...  \n189887  отлично встретили разместили роскошном номере ...  \n189888  васаби ст метро сенная первых ресторан оказалс...  \n189889  ребята стоит смотреть фильм молодые шутливые л...  \n189890  махание руками нигде нашел почему сегодня обос...  \n\n[189891 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>text</th>\n      <th>sentiment</th>\n      <th>without_stopwords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>21098</td>\n      <td>си спросил его о Посланник АллахаТы порицаешь ...</td>\n      <td>1</td>\n      <td>си спросил посланник аллахаты порицаешь чтото ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>21099</td>\n      <td>Роднее всех родных Попала я в ГКБ еще в декабр...</td>\n      <td>1</td>\n      <td>роднее родных попала гкб декабре г ехать больн...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21100</td>\n      <td>Непорядочное отношение к своим работникам Рабо...</td>\n      <td>2</td>\n      <td>непорядочное отношение своим работникам работа...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21101</td>\n      <td>Отсутствуют нормативы Госты и прочее что позво...</td>\n      <td>1</td>\n      <td>отсутствуют нормативы госты прочее позволило о...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>21102</td>\n      <td>У меня машина в руках лет и это первая моя маш...</td>\n      <td>1</td>\n      <td>машина руках лет это первая машина могу никак ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>189886</th>\n      <td>210984</td>\n      <td>Мой юбилей я отмечал в ресторане Астория Этот ...</td>\n      <td>2</td>\n      <td>юбилей отмечал ресторане астория ресторан поль...</td>\n    </tr>\n    <tr>\n      <th>189887</th>\n      <td>210985</td>\n      <td>Отлично встретили разместили в роскошном номер...</td>\n      <td>1</td>\n      <td>отлично встретили разместили роскошном номере ...</td>\n    </tr>\n    <tr>\n      <th>189888</th>\n      <td>210986</td>\n      <td>Была в Васаби на ст метро Сенная Во первых рес...</td>\n      <td>0</td>\n      <td>васаби ст метро сенная первых ресторан оказалс...</td>\n    </tr>\n    <tr>\n      <th>189889</th>\n      <td>210987</td>\n      <td>Ребята не стоит смотреть этот фильм Вы молодые...</td>\n      <td>0</td>\n      <td>ребята стоит смотреть фильм молодые шутливые л...</td>\n    </tr>\n    <tr>\n      <th>189890</th>\n      <td>210988</td>\n      <td>Про махание руками нигде не нашел Почему сегод...</td>\n      <td>1</td>\n      <td>махание руками нигде нашел почему сегодня обос...</td>\n    </tr>\n  </tbody>\n</table>\n<p>189891 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T19:41:25.828811Z",
     "start_time": "2024-03-25T19:41:25.816362Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Лемматизация"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = mystem.lemmatize(' '.join(text).lower())\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "train['lem'] = train['without_stopwords'].apply(lemmatize_text)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-25T20:27:15.666183Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[text.split() for text in train['lem']][0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "sentences = [text.split() for text in train['lem']]\n",
    "\n",
    "# Обучение модели FastText\n",
    "model = FastText(sentences, window=5, min_count=1, workers=4, sg=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Пример применения модели для создания эмбеддингов для всех текстов в столбце\n",
    "embeddings = []\n",
    "for text in train['lem']:\n",
    "    words = text.split()\n",
    "    if words:\n",
    "        # Средний вектор для всех слов в тексте\n",
    "        avg_embedding = sum(model.wv[word] for word in words) / len(words)\n",
    "    else:\n",
    "        # Если список пуст, добавьте нулевой вектор\n",
    "        avg_embedding = [0] * model.vector_size\n",
    "    embeddings.append(avg_embedding)\n",
    "\n",
    "# Добавление в DataFrame нового столбца с эмбеддингами\n",
    "train['embeddings'] = embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Наглядно покажем самые частые слова"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_wordcloud(data, title=None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='black',\n",
    "        max_words=200,\n",
    "        max_font_size=40,\n",
    "        scale=2,\n",
    "        random_state=1\n",
    "    ).generate(\" \".join(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_wordcloud(train['lem'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.to_csv('embs.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Мы решили взять разные модели, основанные на архитектуре трансформер, а также рекуррентные нейронные сети, чтобы дообучить каждую из них и посмотреть, действительно ли трансформер показывает себя лучше, а также выбрать лучшую"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Первая модель - ruBERT, заточеннный под задачу классификации текста, который обучался на данных, схожих с нашими, поэтому можно не трогать слои и просто посмотреть score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip list\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (24.0)\r\n",
      "Requirement already satisfied: setuptools in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (69.2.0)\r\n",
      "Requirement already satisfied: wheel in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (0.43.0)\r\n",
      "Collecting xformers\r\n",
      "  Using cached xformers-0.0.25.tar.gz (4.1 MB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: torch>=2.1 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from xformers) (2.2.1)\r\n",
      "Requirement already satisfied: numpy in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from xformers) (1.23.5)\r\n",
      "Requirement already satisfied: filelock in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=2.1->xformers) (3.9.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=2.1->xformers) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=2.1->xformers) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=2.1->xformers) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=2.1->xformers) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from torch>=2.1->xformers) (2023.10.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from jinja2->torch>=2.1->xformers) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages (from sympy->torch>=2.1->xformers) (1.3.0)\r\n",
      "Building wheels for collected packages: xformers\r\n",
      "  Building wheel for xformers (setup.py) ... \u001B[?25lerror\r\n",
      "  \u001B[1;31merror\u001B[0m: \u001B[1msubprocess-exited-with-error\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[31m×\u001B[0m \u001B[32mpython setup.py bdist_wheel\u001B[0m did not run successfully.\r\n",
      "  \u001B[31m│\u001B[0m exit code: \u001B[1;36m1\u001B[0m\r\n",
      "  \u001B[31m╰─>\u001B[0m \u001B[31m[248 lines of output]\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m running bdist_wheel\r\n",
      "  \u001B[31m   \u001B[0m /Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/torch/utils/cpp_extension.py:500: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\r\n",
      "  \u001B[31m   \u001B[0m   warnings.warn(msg.format('we could not find ninja.'))\r\n",
      "  \u001B[31m   \u001B[0m running build\r\n",
      "  \u001B[31m   \u001B[0m running build_py\r\n",
      "  \u001B[31m   \u001B[0m creating build\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_deprecation_warning.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/attn_bias_utils.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/checkpoint.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/test.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/utils.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_cpp_lib.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/info.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/fused_linear_layer.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/vararg_kernel.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/k_activations.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/k_layer_norm.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/k_fused_matmul_fw.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/dropout.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/k_dropout.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/softmax.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/layer_norm.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/k_fused_matmul_bw.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/triton/k_softmax.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/triton\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/components\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/simplicial_embedding.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/residual.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/reversible.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/activations.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/multi_head_dispatch.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/input_projection.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/patch_embedding.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_mem_eff_attention.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_indexing.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_mlp.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_blocksparse_transformers.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_mem_eff_attn_decoder.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_transformer.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_revnet.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_swiglu.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_triton_layernorm.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_causal_blocksparse.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_triton_fused_linear.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_tiled_matmul.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_triton_blocksparse.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_triton_softmax.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_mem_eff_attention_mqa.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/utils.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_triton_dropout.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_nystrom_utils.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_attn_decoding.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_multi_head_dispatch.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_sequence_parallel_fused.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_sddmm.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_sp24.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/benchmark_core.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/rmsnorm.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/modpar_layers.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/swiglu_op.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/unbind.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/rope_padded.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/seqpar.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/ipc.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/sequence_parallel_fused_ops.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/sp24.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/common.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/differentiable_collectives.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/tiled_matmul.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/indexing.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/profiler\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/profiler/device_limits.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/profiler\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/profiler/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/profiler\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/profiler/profiler_dcgm_impl.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/profiler\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/profiler/profiler_dcgm.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/profiler\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/profiler/api.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/profiler\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/profiler/slow_ops_profiler.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/profiler\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/profiler/profiler.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/profiler\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/sparse\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/sparse/_csr_ops.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/sparse\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/sparse/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/sparse\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/sparse/utils.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/sparse\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/sparse/blocksparse_tensor.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/sparse\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/sparse/csr_tensor.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/sparse\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/helpers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/helpers/test_utils.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/helpers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/helpers/hierarchical_configs.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/helpers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/helpers/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/helpers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/helpers/timm_sparse_attention.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/helpers\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/fused_softmax.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/flash_blocksparse_attention.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/bert_padding.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/flash_attn_triton_og.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/flash_attn_triton.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/flash_attn_interface.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/factory\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/factory/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/factory\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/factory/hydra_helper.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/factory\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/factory/block_factory.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/factory\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/factory/model_factory.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/factory\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/factory/block_configs.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/factory\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/factory/weight_init.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/factory\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/global_tokens.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/ortho.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/blocksparse.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/local.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/compositional.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/pooling.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/_sputnik_sparse.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/core.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/lambda_layer.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/random.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/fourier_mix.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/scaled_dot_product.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/utils.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/attention_mask.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/linformer.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/attention_patterns.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/visual.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/sparsity_config.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/nystrom.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/favor.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/base.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/components/feedforward\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/feedforward/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/feedforward\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/feedforward/mixture_of_experts.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/feedforward\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/feedforward/mlp.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/feedforward\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/feedforward/conv_mlp.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/feedforward\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/feedforward/fused_mlp.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/feedforward\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/feedforward/base.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/feedforward\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/components/positional_embedding\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/positional_embedding/vocab.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/positional_embedding\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/positional_embedding/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/positional_embedding\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/positional_embedding/param.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/positional_embedding\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/positional_embedding/sine.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/positional_embedding\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/positional_embedding/rotary.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/positional_embedding\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/positional_embedding/base.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/positional_embedding\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention/feature_maps\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/feature_maps/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention/feature_maps\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/feature_maps/softmax.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention/feature_maps\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/components/attention/feature_maps/base.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/components/attention/feature_maps\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks/LRA\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/LRA/batch_submit.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks/LRA\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/LRA/batch_fetch_results.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks/LRA\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/LRA/run_with_submitit.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks/LRA\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/LRA/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks/LRA\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/LRA/run_tasks.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks/LRA\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/LRA/run_grid_search.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks/LRA\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks/LRA/code\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/LRA/code/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks/LRA/code\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/LRA/code/model_wrapper.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks/LRA/code\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/benchmarks/LRA/code/dataset.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/benchmarks/LRA/code\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/_triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/_triton/k_scaled_index_add.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/_triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/_triton/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/_triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/_triton/rope_padded_kernels.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/_triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/_triton/sequence_parallel_fused_kernels.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/_triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/_triton/tiled_matmul_kernels.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/_triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/_triton/k_index_select_cat.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/_triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/_triton/rmsnorm_kernels.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/_triton\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/decoder.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/dispatch.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/attn_bias.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/ck.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/common.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/ck_decoder.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/flash.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/small_k.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/cutlass.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/ck_splitk.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/ops/fmha/triton_splitk.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/ops/fmha\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/losses\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/losses/cross_entropy.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/losses\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/losses/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/losses\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/layers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/layers/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/layers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/layers/patch_embed.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/layers\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/layers/rotary.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/layers\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/utils\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/utils/pretrained.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/utils\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/utils/generation.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/utils\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/utils/benchmark.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/utils\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/utils/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/utils\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/utils/distributed.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/utils\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/bigcode.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/gptj.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/opt.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/llama.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/vit.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/btlm.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/baichuan.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/bert.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/falcon.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/gpt_neox.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/models/gpt.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/models\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/activations.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/fused_dense.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/rms_norm.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/layer_norm.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/modules\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/modules/embedding.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/modules\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/modules/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/modules\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/modules/mlp.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/modules\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/modules/block.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/modules\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/modules/mha.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/modules\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/triton/cross_entropy.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/triton/linear.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/triton/k_activations.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/triton/__init__.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/triton/mlp.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/triton/rotary.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops/triton\r\n",
      "  \u001B[31m   \u001B[0m copying xformers/_flash_attn/ops/triton/layer_norm.py -> build/lib.macosx-10.9-universal2-cpython-311/xformers/_flash_attn/ops/triton\r\n",
      "  \u001B[31m   \u001B[0m running build_ext\r\n",
      "  \u001B[31m   \u001B[0m building 'xformers._C' extension\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-universal2-cpython-311\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-universal2-cpython-311/xformers\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-universal2-cpython-311/xformers/csrc\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-universal2-cpython-311/xformers/csrc/attention\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-universal2-cpython-311/xformers/csrc/attention/autograd\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-universal2-cpython-311/xformers/csrc/attention/cpu\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-universal2-cpython-311/xformers/csrc/sequence_parallel_fused\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-universal2-cpython-311/xformers/csrc/sparse24\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-10.9-universal2-cpython-311/xformers/csrc/swiglu\r\n",
      "  \u001B[31m   \u001B[0m clang -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -I/private/var/folders/q4/rd43y_g54plgx59m_wvf7lzr0000gn/T/pip-install-1uh5yqk5/xformers_e30adfb128f94fba8c4b157a22599a20/xformers/csrc -I/Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/torch/include -I/Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/torch/include/torch/csrc/api/include -I/Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/torch/include/TH -I/Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/lib/python3.11/site-packages/torch/include/THC -I/Users/antontagiev/PycharmProjects/text_summarization_sirius/venv/include -I/Library/Frameworks/Python.framework/Versions/3.11/include/python3.11 -c xformers/csrc/attention/attention.cpp -o build/temp.macosx-10.9-universal2-cpython-311/xformers/csrc/attention/attention.o -O3 -std=c++17 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_clang\\\" -DPYBIND11_STDLIB=\\\"_libcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1002\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0\r\n",
      "  \u001B[31m   \u001B[0m clang: error: unsupported option '-fopenmp'\r\n",
      "  \u001B[31m   \u001B[0m clang: error: unsupported option '-fopenmp'\r\n",
      "  \u001B[31m   \u001B[0m error: command '/usr/bin/clang' failed with exit code 1\r\n",
      "  \u001B[31m   \u001B[0m \u001B[31m[end of output]\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[1;35mnote\u001B[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "\u001B[31m  ERROR: Failed building wheel for xformers\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[?25h  Running setup.py clean for xformers\r\n",
      "Failed to build xformers\r\n",
      "\u001B[31mERROR: Could not build wheels for xformers, which is required to install pyproject.toml-based projects\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "!pip install xformers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T13:28:11.014334Z",
     "start_time": "2024-03-25T13:28:04.164719Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"MonoHime/rubert-base-cased-sentiment-new\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T13:30:04.925739Z",
     "start_time": "2024-03-25T13:30:01.389816Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Обрежем текст, чтобы умещаться в данное количество токенов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "test['text'] = test['text'].apply(lambda text: text[:1024])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T13:30:12.415357Z",
     "start_time": "2024-03-25T13:30:12.299506Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "0        Развода на деньги нет Наблюдаюсь в Лайфклиник ...\n1        Отель выбрали потому что рядом со стадионом. О...\n2        Вылечили Гноился с рождения глазик, в поликлин...\n3        Хорошее расположение.С вокзала дошли пешком.Но...\n4        Отличное месторасположение,прекрасный вид,особ...\n                               ...                        \n21093    Несколько лет назад муж останавливался в этом ...\n21094    Спасли от боли После родов у меня появились бо...\n21095    з ролика понятно одно - девушка- наблюдатель  ...\n21096    Остались всем довольны, дружелюбный персонал, ...\n21097    Спасибо огромное за сыночка Хочу выразить огро...\nName: text, Length: 21098, dtype: object"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T13:30:14.470260Z",
     "start_time": "2024-03-25T13:30:14.444889Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import razdel\n",
    "# import re\n",
    "#\n",
    "#\n",
    "# # Функция для токенизации текста с использованием библиотеки razdel\n",
    "# def tokenize_text(text):\n",
    "#     tokens = [token for token in razdel.tokenize(text)]\n",
    "#     return tokens\n",
    "#\n",
    "#\n",
    "# # Применяем токенизацию сначала к тексту, а затем к результатам первой токенизации\n",
    "# test['refact_text'] = test['text'].apply(lambda tokens: ''.join(tokens))  # объединяем списки токенов обратно в текст\n",
    "# test['refact_text'] = test['refact_text'].apply(tokenize_text)  # применяем токенизацию с использованием razdel\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def extract_words(tokens):\n",
    "#     words = [token.text for token in tokens if token.text.isalpha()]\n",
    "#     return words\n",
    "#\n",
    "#\n",
    "# test['refact_text'] = test['refact_text'].apply(extract_words)\n",
    "# test['refact_text']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r'[^\\w\\s]|\\d|http\\S+'\n",
    "\n",
    "# Применяем регулярное выражение ко всему столбцу 'Text'\n",
    "test['text'] = test['text'].apply(lambda x: re.sub(pattern, '', x))\n",
    "test['refact_text'] = test['text'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test['refact_text'] = test['refact_text'].apply(\n",
    "    lambda tokens: [word.lower() for word in tokens if word.lower() not in stop_words and word != ''])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test['refact_text']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    result = pipe(text)[0]  # Получение результата анализа текста\n",
    "    return result['label']  # Возвращение метки настроения\n",
    "\n",
    "\n",
    "# Применение функции к каждой строке в столбце 'text'\n",
    "test['sentiment'] = test['refact_text'].apply(predict_sentiment)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test['sentiment']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test['sentiment'] = test['sentiment'].str.replace('NEGATIVE', '2')\n",
    "test['sentiment'] = test['sentiment'].str.replace('POSITIVE', '1')\n",
    "test['sentiment'] = test['sentiment'].str.replace('NEUTRAL', '0')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = test.drop('text', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Набранный F1-score на AllCups - 0.60"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = result.drop('refact_text', axis=1)\n",
    "result.to_csv('../outputs/from_pretrained2.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Сохраним эту модельку как неплохой вариант и пойдем дальше"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(pipe, '../models/pretrained_model.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Далее мы решили дообучить модель BERT для анализа тональности, потому что обычно она справляется с задачами классификации лучше, чем GPT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Код с дообучением находится в файле fine-tuning-bert.ipynb, обучить модель не получилось, поскольку это потребовало значительных ресурсов. Обучение шло на 4090 с 64 гигабайтами оперативной памяти, однако этого оказалось мало."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выходом из сложившейся ситуации оказалась библиотека LightAutoML от сбера, на базе которой мы развернули уже новую модель параллельно строя ROC-кривую"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Дальнейшее продолжение в файле 'Christian's automl.ipynb'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:57:00.219600Z",
     "start_time": "2024-03-26T12:57:00.213766Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:57:01.534469Z",
     "start_time": "2024-03-26T12:57:01.530517Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:51:15.241100Z",
     "start_time": "2024-03-26T12:51:15.239875Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:39:07.779794Z",
     "start_time": "2024-03-26T12:39:07.776555Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T12:51:17.231615Z",
     "start_time": "2024-03-26T12:51:17.019789Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
